---
title: "classify_otherruns_coi"
author: "Eily Allan - modified from Erin D'Agnese and Ramon Gallego"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
params: 
  folder:
  run:
    value: 6
  Hash_key:
    value: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_EA/Output/dada2_output/run6_20211229/COI/hash_key.csv
  ASVs: 
    value: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_EA/Output/dada2_output/run6_20211229/COI/ASV_table.csv
  previous_good_effort: 
    value: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_EA/Output/classification_output/COI.all.good.previous.hashes.annotated.rds
  previous_effort: 
    value: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_EA/Output/classification_output/COI.all.previous.hashes.annotated.rds
---

Last updated: 2/2/22

# Overview

This code is meant to take output from dada2 and assign taxonomy. This is adapted from Ramon Gallego's "insect.all.Rmd" script found here (https://github.com/ramongallego/eDNA.and.Ocean.Acidification.Gallego.et.al.2020/tree/master/Scripts). 

The general overview is that ASVs from dada2 will be read in, we will find hashes that were already previously annotated from other runs and pull them out (because we do not need to re-assign taxonomy). For ASVs that have not yet been classified, we classify using insect (found here - https://cran.r-project.org/web/packages/insect/vignettes/insect-vignette.html). 

Then we will add the newly classified hashes via insect to the previous effort to keep growing our list of classified things.
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = params$folder)
```

# Set uo

## Load libraries and set file paths, etc.

```{r load libraries, echo = FALSE}
library (tidyverse)
library (insect)
library (seqinr)
library (here)
library(taxonomizr)

marker <- "COI"
# note that the classifier is stored LOCALLY so need to change file path 
classifier <- "/Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_LOCAL/Input/classifiers/classifier_COI_v5.rds"

run_output_folder <- paste0(here("Output","classification_output"),"/run",params$run) 
dir.create(path = run_output_folder)
run_marker_output_folder <- paste0(run_output_folder,"/",params$marker)
dir.create(path = run_marker_output_folder)

```


# Step 0: Read in files and manipulate as needed. 

## Read in ASV table, hash key, previous effort, and classification tree

```{r read in files}
Hash     <- read_csv(params$Hash_key) %>% 
  select(Hash, Sequence) %>% distinct()
ALL.ASVs <- read_csv(params$ASVs)

previous.good.effort <- read_rds(params$previous_good_effort)
previous.effort <- read_rds(params$previous_effort)

tree <- read_rds(classifier)

```

## For COI, see if you need to RC. 
Now we are idiots and for runs 2, 3, and 4 - some samples got COI primers put on backwards and some were put on forwards. So we need to RC the backwards ones in order for them to be classified by insect. 

```{r coi backwards issue}

# ### COI -- HAS SOME BACKWARDS
# # #reverse complement the representative sequence for each hash
# cutadapt_metadata <- read.csv("/Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_LOCAL/Output/cutadapt_output/run4_20211117/noprimers/COI/cutadapt_output_metadata_COI.csv")
# is.backwards <- (cutadapt_metadata$rc == 1)
# samples.backwards <- cutadapt_metadata$Sample_name[is.backwards]
# samples.forwards <- cutadapt_metadata$Sample_name[! is.backwards]
# 
# ASVs.fwd <- ALL.ASVs %>%
#   filter(Sample_name %in% samples.forwards)
# ASVs.bkwd <- ALL.ASVs %>%
#   filter(Sample_name %in% samples.backwards)
# 
# Hash.fwds <- Hash %>%
#   filter(Hash %in% ASVs.fwd$Hash)
# Hash.bkwds <- Hash %>%
#   filter(Hash %in% ASVs.bkwd$Hash)
# 
# # actually RC them
# rc_seq <- vector(length = dim(Hash.bkwds)[1])
# # this is ridiculous and shouldn't be for loop but wtf i can't get it to work
# for (i in 1:dim(Hash.bkwds)[1]) {
#   rc_seq[i] = rc(Hash.bkwds[i,2])
# }
# Hash.bkwds$Sequence <- rc_seq
# Hash.bkwds <- Hash.bkwds %>%
#   select(Hash, Sequence) %>% distinct()
# 
# Hash <- rbind(Hash.fwds, Hash.bkwds)

```


# Step 1: Don't reinvent the wheel. Remove ASVs that we previously classified.

We expect to find hashes that are the same as some of our previous runs - so we don't need to re-classify them. We can just see what insect classified them as the first time around. Since we want the best classifications possible, let's only compare to our "good" previous effort. Then we will classify any hashes that were previously assigned something but not below family level again to see if we get a better resolution. 

```{r make it into a DNA object}

## here we are going to separate out the ASVs that are NOT in our previous effort file (i.e., only classify things that did not get a good classification previously)
# we probably only want to use the good classifications from previous - but you could change this to params$previous_effort
new.set <- anti_join(Hash, previous.good.effort, by = c("Hash" = "representative")) # remove anything previously classified
new.hashes.insect <- char2dna(new.set$Sequence)
names(new.hashes.insect) <- new.set$Hash

# just check it out 
new.hashes.insect

```

# Step 2: Now classify all the new things with the insect classifier and save them. 

## Classify

Now we only have ASVs that we know nothing about yet.
Let's classify these new sequences in our bin file.

```{r classify}
clasif.hashes <- classify (x = new.hashes.insect, tree = tree, cores = 4)

names(clasif.hashes) <- c('representative', 'taxID', 'taxon', 'rank', 'score', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

clasif.hashes %>% 
  unite (family, genus, species, sep = "|", col = "taxa")
clasif.hashes %>% dplyr::count (rank) %>% arrange(desc(n))
```

## Check them out
Now that we have classifications, let's start looking at the results and see how well the classifier did.

```{r check out results and manipulate a little}

# look at family/genus/species classifications
clasif.hashes %>% 
  unite (family, genus, species, sep = "|", col = "taxa")

# see how many were not assigned a rank
clasif.hashes %>% dplyr::count (rank) %>% arrange(desc(n))

# How many have a valid family but no phylum info
clasif.hashes %>% 
  filter(family!= "" & phylum == "") %>% 
  distinct(class) 

# Add new phylum info
clasif.hashes %>% 
  mutate(phylum = case_when(phylum != "" ~ phylum,
                            TRUE   ~ class))

```

## Save them all 
OK. So now let's save the new classifications from just this run as an RDS and CSV. We are actually going to save two versions of this - one that is all hashes that are classified to anything (to get the most information possible) - but then also just "good" classifications (so that we don't save a not great classification and then never go back and try to classify it again with insect or BLAST). 

```{r save new classifications}

# first, let's save it in a subfolder of Output/classification_output/ and the run - so we know what was classified only from this run
# that is our "run_marker_output_folder" file path that we defined above
# here we will save it as both an rds and a csv 
saveRDS(clasif.hashes, paste0(run_marker_output_folder,"/new.hashes.annotated.rds"))
clasif.hashes <- readRDS(paste0(run_marker_output_folder,"/new.hashes.annotated.rds"))
write.csv(clasif.hashes, paste0(run_marker_output_folder,"/new.hashes.annotated.csv"))

# also write them as a tax table in the same place
source(here("functions", "tax.table.R"))
taxtable <- tax.table(clasif.hashes)
write.csv(taxtable,file=paste0(run_marker_output_folder,"/new.tax.table.csv"))

```

## Save just the good ones
What we call "good" is going to be different for COI vs. 12S. For 12S, we really want to get to species level so we will only call species level IDs "good". For COI, we don't expect to get to species level for most things because the marker is so general/universal. So... let's start with calling anything to family level "good" and then go from there. 

```{r save only good new classifications}

# but let's only save the "good ones" (FAMILY  for COI) for this run
good.clasif.hashes <- clasif.hashes %>% filter(rank == "family")

# first, let's save it in a subfolder of Output/classification_output/ and the run - so we know what was classified from this run
# that is our "run_marker_output_folder" file path that we defined above
# here we will save it as both an rds and a csv 
saveRDS(good.clasif.hashes, paste0(run_marker_output_folder,"/new.good.hashes.annotated.rds"))
clasif.hashes <- readRDS(paste0(run_marker_output_folder,"/new.good.hashes.annotated.rds"))
write.csv(clasif.hashes, paste0(run_marker_output_folder,"/new.good.hashes.annotated.csv"))

# also write them as a tax table in the same place
source(here("functions", "tax.table.R"))
taxtable <- tax.table(good.clasif.hashes)
write.csv(taxtable,file=paste0(run_marker_output_folder,"/new.good.tax.table.csv"))

```

# Step 3: And then add the new classifications to make a new "previous effort".

Now that we saved the new ones that we classified in this run, we want to add them to our previous effort and build our local database of annotated ASVs. (We will do this for any annotation and just "good" ones.)

## Combine all previous and all new.

```{r combine and save all classifications}
# make and save combined rds file with the previous effort and the new one 
combined <- rbind(previous.effort, clasif.hashes)
combined.taxtable <- tax.table(combined)

# we want to put the combined / new previous hashes in the general Output/classification_output folder that we can re-write over and over again as we add new classifications each time that we add another run 

# so here we are going to change the file path - and the file name to say "all.previous.hashes" - and let's also add the date on the end of the file name so we know when it was last updated - this will be slightly annoying later when we have to change the file name when we read in the classifications, but probably worth it so we keep track of things
# save it again as an rds and then also as a csv - and tax table
saveRDS(combined, file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,".all.previous.hashes.annotated.rds"))
combined <- readRDS(file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,".all.previous.hashes.annotated.rds"))
write.csv(combined, file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,".all.previous.hashes.annotated.csv"))
write.csv(combined.taxtable,file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,"combined.tax.table.csv"))

```

## Combine and save just the good previous and new. 

```{r combine and save just good}

combined.good <- rbind(previous.good.effort,good.clasif.hashes)
combined.good.taxtable <- tax.table(combined.good)

# we want to put the combined / new previous hashes in the general Output/classification_output folder that we can re-write over and over again as we add new classifications each time that we add another run 

# so here we are going to change the file path - and the file name to say "all.previous.hashes" - and let's also add the date on the end of the file name so we know when it was last updated - this will be slightly annoying later when we have to change the file name when we read in the classifications, but probably worth it so we keep track of things
# save it again as an rds and then also as a csv - and tax table
saveRDS(combined.good, file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,".all.good.previous.hashes.annotated.rds"))
combined.good <- readRDS(file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,".all.good.previous.hashes.annotated.rds"))
write.csv(combined.good, file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,".all.good.previous.hashes.annotated.csv"))
write.csv(combined.good.taxtable,file=paste0(here("Output","classification_output"),"/",format(Sys.Date(), "%Y%m%d"), ".", params$marker,"combined.good.tax.table.csv"))

```

Let's stop here for now - later, we can go back and blast any "not good" classifications to see how much more information we get. But for now, let's just get the pipeline up and running and get the majority of things classified. Later we can (hopefully) easily add in a step where we take anything that is not yet classified and BLAST it to squeeze every bit of information we can out of the data. 

We could even do this after we classify ALL the runs and then just have one large blast script with all ASVs that don't get annotated to only BLAST once instead of 10+ times. 

