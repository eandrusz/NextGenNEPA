---
title: "fastqs_to_asvs"
author: "Eily Allan"
date: "7/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fastqs to ASVs - Overview

This script takes in raw fastq files from a MiSeq sequencing run and uses cutadapt to remove primers and dada2 to trim reads based on quality scores, merge paired end reads, and make ASVs (using hashes) from reads. (This was really done by Ramon Gallego - see  https://github.com/ramongallego/Nextera_Dada2 -- we have just rearranged it and tweaked it for our purposes.)

This is designed to work with sequencing data that has been prepared by using dual-indexed primers (link) and sequencing runs that have multiple markers on the same run.

Let's load some libraries we will need:
```{r load packages}
library(tidyverse)
library(here)
library (dada2)
library (digest)
library (seqinr)
library (knitr)
library (kableExtra)
```

## Step 1: Remove indices and primers from fastq files. 

First, we will use cutadapt to remove the Nextera indices and PCR primers so we are only left with fasta files of our fragment(s) of interest. 

We need the following inputs: 
- path to folder where the fastq files live 
- a csv with the information about the Nextera index and the PCR marker for each sample - these are the MINIMUM requirements (more columns are ok)
        - Sample_name - The name that makes sense to you and your project (No spaces in the name would be better)
        - Locus: The name of the locus you want to use (e.g. Leray_COI)
        - PrimerF: The nucleotide sequence of the forward primer - supports IUPAC characters 
        - PrimerR: Ditto for the reverse primer (also in 5' -> 3' direction)
        - file1: it should match exactly the output of the Miseq.
        - file2: Same for the second read.

We also need to specify where we want the output to go:
- path to folder where the output will go, which will include:
        - a subfolder that includes the trimmed fasta files 
        - a csv with the parameters used in cutadapt

Finally, we need to tell cutadapt:
- the minimum length to trim


Let's start by setting paths for inputs/outputs and creating the parameter file:
```{r set paths and create parameter file}

# paths for inputs/outputs
path_cutadapt_input_fastas <- here("Data","fastq_files","20210527_prelim_run")
path_cutadapt_input_metadata <- here("Data","sequencing_metadata_files","20210527_prelim_run_metadata_sub.csv")
path_cutadapt_output_folder <- here("Output","cutadapt_output_files","20210527_prelim_run_sub")

# set minimum length to trim
input_cutadapt_min_trim_length <- 100

# write these all as parameters
cutadapt_params = list(
  folder = path_cutadapt_input_fastas,
  metadata = path_cutadapt_input_metadata, 
  outputfolder = path_cutadapt_output_folder,
  minlength = input_cutadapt_min_trim_length)

# create directory where outputs (cleaned fasta files and csv of parameters) will go
dir.create(cutadapt_params$outputfolder)

# create parameter output file 
tibble(values = as.character(cutadapt_params), names = as.character(names(cutadapt_params))) %>% 
  pivot_wider(names_from = names,
              values_from = values) %>%
  select(folder, metadata, outputfolder, minlength) %>% 
write_csv( here("Output","cutadapt_output_files","20210527_prelim_run_sub","cutadapt_params.txt") )

#setwd(here("In_Progress")) 
```

If everything looks good (double check the metadata file columns), then go ahead and start. We actually aren't using cutadapt in R - instead we are passing the parameters from the metadata file into a bash script. In other words, this is a wrapper for cutadapt.

This means that we need to have the following shell script in our file path "test.bash.sh"

Let's check the version of cutadapt we are using. First we need to change our file path

```{r change file path for cutadapt}
# if this doesn't work (command not found), use the following:
old_path <- Sys.getenv("PATH")
Sys.setenv(PATH = paste(old_path, "/Users/elizabethandruszkiewicz/opt/anaconda2/bin/", sep = ":"))
```

Then check cutadapt version
```{bash check cutadapt version}
cutadapt --version
```

Next we will actually push the parameters to the bash script that runs cutadapt.
```{bash run cutadapt}
bash test.bash.sh cutadapt_params.txt
```

## Step 2: dada2

Now that we have removed indices and PCR primers, we have a folder of fastas with only our fragments of interest. Now we are going to use dada2 (https://github.com/benjjneb/dada2) to look at quality scores, trim based on quality scores, merge paired end reads, and then organize reads into ASVs (amplicon sequence variants). 

We need to set the file paths for inputs and outputs again and then set a few more parameters. One parameter is whether to use Hashes or not - we should always use hashes so that we can compare ASVs across different sequencing runs to see if the same sequence is found in different samples. 

We will also set a parameter for the trimming length for each of the two files (F and R reads - or R1 and R2). This will change based on marker. Our original metadata file that we used for cutadapt has a column specifying which marker was used. Here we will create a new column to give the trimming length based on which marker was used for PCR. 

```{r parameters for dada2}

# paths for inputs/outputs
path_dada2_input_fastas <- here("Data","fastq_files","20210527_prelim_run_sub","noprimers")
path_dada2_input_metadata <- here("Output","cutadapt_output_files", "20210527_prelim_run_sub", "noprimers", "output.metadata.csv")
path_dada2_output_folder <- here("Output","dada2_output_files","20210527_prelim_run_sub")

# parameters
input_dada2_hash <- TRUE
input_data2_keep_mid_files <- TRUE
input_dada2_R1_trim_length <- 200
input_dada2_R2_trim_length <- 200

# set minimum length to trim
input_min_trim_length <- 200

# write these all as parameters
dada2_params = list(
  folder = path_dada2_input_fastas,
  hash = input_dada2_hash,
  trimming.length.Read1 = input_dada2_R1_trim_length,
  trimming.length.Read2 = input_dada2_R2_trim_length,
  metadata = path_dada2_input_metadata, 
  output.folder = path_dada2_output_folder,
  keep.mid.files = input_data2_keep_mid_files)

# now we will actually create the 
sample.metadata <- read_csv(dada2_params$metadata)
filt_path <- file.path(dada2_params$folder, "filtered")
getwd()
filt_path

# Check if output directory exists - if not create as a subfolder of input dir
if(!dir.exists(file.path(dada2_params$output.folder))){
  dir.create(path = file.path(dada2_params$output.folder),recursive = T)
  output.dir <- file.path(dada2_params$output.folder)
}else{
  output.dir <- file.path(dada2_params$output.folder)
}
output.dir

# Write the parameters in a file to the output dir
tibble(Parameters = names(dada2_params), Values = unlist(dada2_params)) %>% write_csv(file.path(output.dir,paste0("dada2_parameters_", Sys.Date(), ".csv")))
```

Now we want to look at the quality scores of our reads (after removing indices and PCR primers)

```{r check quality trim point, message=FALSE, warning=FALSE}


ifelse(nrow(sample.metadata)>4,
       subset <- sample.metadata %>%  sample_n(4),
       subset <- sample.metadata)

subset %>% pull(file1) %>%
  plotQualityProfile(.)

subset %>% pull(file2) %>%
  plotQualityProfile(.)

```
