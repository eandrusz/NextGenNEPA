---
title: "Dada2_report"
author: "Eily via Moncho"
date: "2/26/2022`"
output:
  html_notebook:
    toc: true
params:
  noprimerfolder: 
    value: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_LOCAL/Output/cutadapt_output/run5_20220226/noprimers/Ac16S
  hash: TRUE
  trimming.length.Read1: 250
  trimming.length.Read2: 200
  metadata: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_LOCAL/Output/cutadapt_output/run5_20220226/noprimers/Ac16S/cutadapt_output_metadata_Ac16S.csv
  output.folder: /Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA/Output/dada2_output/run5_20220226/Ac16S
  keep.mid.files: FALSE

---

Last updated: 2/2/22

# Overview

After removing primers by cutadapt and splitting primers into subfolder, we are going to apply a denoising algorithm `DADA2`, https://github.com/benjjneb/dada2 to generate ASV tables and hash keys. On the parameters section you have chosen a trimming length for each of the two files, whether to use Hashes or not, and a folder where to drop the output files. 

The trimming length depends on the marker and the run. In general, for a paired end 2x300 run, we use trimming lengths of:
MiFish/MiMammal: 150/150
COI: 250/200

This is because our 12S (MF/MM) fragment is only ~170 bp long and our COI (Leray XT) fragment is ~313 bp long. These should not be blindly used, but instead, after plotting the Q score plots for a subset (or aggregate) samples they should be revisited. The Q score plots vary for each run and if the run is poor you should adjust to have these be shorter and if it is a great run you can extend the trimming lengths for the COI marker to get more overlap. The rule of thumb is to trim when Q scores drop below 30. So go back to the parameters after running the chunk of plotting Q scores. 


# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = params$noprimerfolder)

```

## Load packages and set file paths 

```{r loading packages, echo=TRUE ,message=FALSE}
library(here)
library (tidyverse)
library (dada2)
library (digest)
library (seqinr)
library (knitr)
library (kableExtra)

sample.metadata <- read_csv(params$metadata)
filt_path <- file.path(params$noprimerfolder, "filtered")
getwd()
filt_path
# Check if output directory exists - if not create as a subfolder of input dir
if(!dir.exists(file.path(params$output.folder))){
  dir.create(path = file.path(params$output.folder),recursive = T)
  output.dir <- file.path(params$output.folder)
}else{
  output.dir <- file.path(params$output.folder)
}
output.dir
# Write the parameters in a file to the output dir
tibble(Parameters = names(params), Values = unlist(params)) %>% write_csv(file.path(output.dir,paste0("parameters_", Sys.Date(), ".csv")))

```

## QC - remove any bad samples 

*Be careful here to make sure you comment out and don't by accident drop good samples!*

We need to remove samples that didn't get past cutadapt - have to change this EACH TIME and you don't want it uncommented if you don't need it!! 

```{r remove any that didnt get past cutadapt}

## removed any samples removed during filtering so edit metadata file to remove it
#Locus_MiFish_MiFish-0621-4Pad-Up11-3_S45_L001_R1_001.fastq)
#Locus_MiMammal_MiMammal-0721-4Pad-Up11-2_S19_L001_R1_001.fastq
#Locus_Ac16S_Ac16S-PadUp11-4-0321_S113_F1_filt.fastq.gz
sample.metadata <- sample.metadata[-1,]

```

## QC - check quality score plots and change trimming lengths as needed

*Also important to make sure you go back and change the parameters if you need to* 

It is probably fine to just plot a subset of you samples (4) and check Q score plots. You can also plot the whole aggregate R1 and R2 Q scores (this will take longer to run.) See where Q scores start dropping below 30 to determine trimming length. Change parameters if you need to!! 

```{r check quality trim point, message=FALSE, warning=FALSE}

# create a subset of four files to plot Q scores for
ifelse(nrow(sample.metadata)>4,
       subset <- sample.metadata %>%  sample_n(4),
       subset <- sample.metadata)

subset %>% pull(file1) %>%
  plotQualityProfile(.)
ggsave(file.path(output.dir,"R1_subset.png"))

subset %>% pull(file2) %>%
  plotQualityProfile(.)
ggsave(file.path(output.dir,"R2_subset.png"))

# plot aggregate - takes a while so comment out if you don't want

sample.metadata %>% pull(file1) %>%
  plotQualityProfile(., aggregate=TRUE)
ggsave(file.path(output.dir,"R1_aggregate.png"))

sample.metadata %>% pull(file2) %>%
 plotQualityProfile(., aggregate=TRUE)
ggsave(file.path(output.dir,"R2_aggregate.png"))

```

# Run dada2 

## 

```{r dadaing}

### SOME OF COI IS BACKWARDS

sample.metadata %>%
  #filter(rc == 1) %>% # ONLY SELECT THE BACKWARDS ONES (1) OR FORWARDS ONES (0)
  separate(file1, into = "basename", sep= "_L001_R1", remove = F) %>% 
  mutate(filtF1 = file.path("filtered", paste0(basename, "_F1_filt.fastq.gz")),
         filtR1 = file.path("filtered", paste0(basename, "_R1_filt.fastq.gz"))) %>%
  select(-basename) %>% 
  mutate (outFs = pmap(.l= list (file1, filtF1, file2, filtR1),
                       .f = function(a, b, c, d) {
                         filterAndTrim(a,b,c,d,
                                       truncLen=c(params$trimming.length.Read1,params$trimming.length.Read2),
                                       maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                                       compress=TRUE, multithread=TRUE )
                       } ),
          errF1 = map(filtF1, ~ learnErrors(.x, multithread=TRUE,verbose = 0)),     # Calculate errors
          errR1 = map(filtR1, ~ learnErrors(.x, multithread=TRUE,verbose = 0)),
          derepF1 = map(filtF1, derepFastq),                   # dereplicate seqs
          derepR1 = map(filtR1, derepFastq),
          dadaF1  = map2(derepF1,errF1, ~ dada(.x, err = .y, multithread = TRUE)),  # dada2
          dadaR1  = map2(derepR1,errR1, ~ dada(.x, err = .y, multithread = TRUE)),
          mergers = pmap(.l = list(dadaF1,derepF1, dadaR1,derepR1),                 # merge things
                         .f = mergePairs )) -> output.dada2
if ( params$keep.mid.files==TRUE){
write_rds(output.dada2, path = "output.halfway.rds")}


seqtabF <- makeSequenceTable(output.dada2$mergers)
dim(seqtabF)

table(nchar(getSequences(seqtabF)))

```

Now we will remove chimera sequences. This is done within dada2 but you could use other ways

```{r RemovingChimeras, message=F}
seqtab.nochim <- removeBimeraDenovo(seqtabF, method="consensus", multithread=TRUE)
dim(seqtab.nochim)
seqtab.nochim.df <- as.data.frame(seqtab.nochim)
```


```{r setting up output files}
# Copy the metadata so it is all in one place
sample.metadata %>% write_csv(file.path(output.dir,"metadata.csv"))

# Output files
  conv_file <- file.path(output.dir,"hash_key.csv")
  conv_file.fasta <- file.path(output.dir,"hash_key.fasta")

  ASV_file <-  file.path(output.dir,"ASV_table.csv")

  print (conv_file)
  print (conv_file.fasta)
  print(ASV_file)
```

```{r Hash or not}
if ( params$hash==TRUE)
  {
  conv_table <- tibble( Hash = "", Sequence ="")


   map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))
  
  colnames(seqtab.nochim.df) <- Hashes


  
  write_csv(conv_table, conv_file) # write the table into a file
  write.fasta(sequences = as.list(conv_table$Sequence),
              names     = as.list(conv_table$Hash),
              file.out = conv_file.fasta)
 seqtab.nochim.df <- bind_cols(sample.metadata %>%
                                select(Sample_name, Locus),
                              seqtab.nochim.df)
seqtab.nochim.df %>%
  pivot_longer(cols = c(- Sample_name, - Locus),
              names_to = "Hash",
              values_to = "nReads") %>%
  filter(nReads > 0) -> current_asv
write_csv(current_asv, ASV_file)    }else{
  #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
   seqtab.nochim.df %>%
  pivot_longer(cols = c(- Sample_name, - Locus),
              names_to = "Sequence",
              values_to = "nReads") %>%
  filter(nReads > 0) -> current_asv
write_csv(current_asv, ASV_file)
}
```


## Track the fate of all reads

```{r output_summary}
getN <- function(x) sum(getUniques(x))

output.dada2 %>%
  select(-file1, -file2, -filtF1, -filtR1, -errF1, -errR1, -derepF1, -derepR1) %>%
  mutate_at(.vars = c("dadaF1", "dadaR1", "mergers"),
            ~ sapply(.x,getN)) %>%
#  pull(outFs) -> test
  mutate(input = map_dbl(outFs, ~ .x[1]),
         filtered = map_dbl(outFs, ~ .x[2]),
         tabled  = rowSums(seqtabF),
         nonchim = rowSums(seqtab.nochim)) %>%
  select(Sample_name,
         Locus,
         input,
         filtered,
         denoised_F = dadaF1,
         denoised_R = dadaR1,
         merged = mergers,
         tabled,
         nonchim) -> track

write_csv(track, file.path(output.dir,"dada2_summary.csv"))
```

```{r drop}
if ( params$keep.mid.files==FALSE){
unlink(filt_path, recursive = T)
  }
```


```{r output_summary table and fig}
kable (track, align= "c", format = "html") %>%
      kable_styling(bootstrap_options= "striped", full_width = T, position = "center") %>%
      column_spec(2, bold=T)


```


```{r output_summary plot}
track %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`, group =  Sample_name, color = Sample_name)) +
  geom_line() +
  facet_wrap(~Sample_name) +
  guides(color = "none")

```
